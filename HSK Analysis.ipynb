{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a6043e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import pinyin\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac60673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hsk_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bb2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hsk1 = df[df['Level'] == '1']\n",
    "df_hsk2 = df[df['Level'] == '2']\n",
    "df_hsk3 = df[df['Level'] == '3']\n",
    "df_hsk4 = df[df['Level'] == '4']\n",
    "df_hsk5 = df[df['Level'] == '5']\n",
    "df_hsk6 = df[df['Level'] == '6']\n",
    "df_hsk7_9 = df[df['Level'] == '7-9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58a13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsk1_hanzi_set = set(list(''.join([word for word in (df_hsk1['Simplified'] + df_hsk1['Traditional'])])))\n",
    "hsk2_hanzi_set = set(list(''.join([word for word in (df_hsk2['Simplified'] + df_hsk2['Traditional'])])))\n",
    "hsk3_hanzi_set = set(list(''.join([word for word in (df_hsk3['Simplified'] + df_hsk3['Traditional'])])))\n",
    "hsk4_hanzi_set = set(list(''.join([word for word in (df_hsk4['Simplified'] + df_hsk4['Traditional'])])))\n",
    "hsk5_hanzi_set = set(list(''.join([word for word in (df_hsk5['Simplified'] + df_hsk5['Traditional'])])))\n",
    "hsk6_hanzi_set = set(list(''.join([word for word in (df_hsk6['Simplified'] + df_hsk6['Traditional'])])))\n",
    "hsk7_9_hanzi_set = set(list(''.join([word for word in (df_hsk7_9['Simplified'] + df_hsk7_9['Traditional'])])))\n",
    "hsk_hanzi_set = set(list(''.join([word for word in (df['Simplified'] + df['Traditional'])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c4f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsk1_word_set = set(list(df_hsk1['Simplified']) + list(df_hsk1['Traditional']))\n",
    "hsk2_word_set = set(list(df_hsk2['Simplified']) + list(df_hsk2['Traditional']))\n",
    "hsk3_word_set = set(list(df_hsk3['Simplified']) + list(df_hsk3['Traditional']))\n",
    "hsk4_word_set = set(list(df_hsk4['Simplified']) + list(df_hsk4['Traditional']))\n",
    "hsk5_word_set = set(list(df_hsk5['Simplified']) + list(df_hsk5['Traditional']))\n",
    "hsk6_word_set = set(list(df_hsk6['Simplified']) + list(df_hsk6['Traditional']))\n",
    "hsk7_9_word_set = set(list(df_hsk7_9['Simplified']) + list(df_hsk7_9['Traditional']))\n",
    "hsk_word_set = set(list(df['Simplified']) + list(df['Traditional']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a35593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pinyin(word):\n",
    "    if word in word_pinyin_map.keys()\n",
    "\n",
    "\n",
    "# for a given hanzi, return the lowest level that it appears in\n",
    "def get_hanzi_level(hanzi):\n",
    "    if hanzi in hsk1_hanzi_set:\n",
    "        return '1'\n",
    "    if hanzi in hsk2_hanzi_set:\n",
    "        return '2'\n",
    "    if hanzi in hsk3_hanzi_set:\n",
    "        return '3'\n",
    "    if hanzi in hsk4_hanzi_set:\n",
    "        return '4'\n",
    "    if hanzi in hsk5_hanzi_set:\n",
    "        return '5'\n",
    "    if hanzi in hsk6_hanzi_set:\n",
    "        return '6'\n",
    "    if hanzi in hsk7_9_hanzi_set:\n",
    "        return '7-9'\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a81c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of characters where each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8fc68f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hanzi(char):\n",
    "    # hanzi are between U+4E00 and U+9FFF\n",
    "    return ord(char) <= 40959 and ord(char) >= 19968\n",
    "\n",
    "\n",
    "def is_all_hanzi(string):\n",
    "    return all([is_hanzi(char) for char in list(string)])\n",
    "\n",
    "# this function was written by ChatGPT\n",
    "def escape_regex(string):\n",
    "    regex_reserved_chars = r'\\^$.|?*+(){}[]'\n",
    "    escaped_string = re.sub(f'([{re.escape(regex_reserved_chars)}])', r'\\\\\\1', string)\n",
    "    return escaped_string\n",
    "\n",
    "\n",
    "# split text into chunks of Hanzi split by non-Hanzi characters\n",
    "def split_into_hanzi_chunks(text):\n",
    "    if is_all_hanzi(text):\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    non_hanzi_characters = []\n",
    "    \n",
    "    for char in list(text):\n",
    "        if not is_hanzi(char):\n",
    "            non_hanzi_characters.append(escape_regex(char))\n",
    "    \n",
    "    regex_split_string = '|'.join(list(set(non_hanzi_characters)))\n",
    "    split_string = re.split(regex_split_string, text)\n",
    "    \n",
    "    for string in split_string:\n",
    "        if len(string) > 0:\n",
    "            chunks.append(string)\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# reverse of the function above\n",
    "def split_into_non_hanzi_chunks(text):\n",
    "    chunks = []\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    curr_chunk = ''\n",
    "    \n",
    "    for char in list(text):\n",
    "        if not is_hanzi(char):\n",
    "            curr_chunk += char\n",
    "        if is_hanzi(char) and curr_chunk != '':\n",
    "            chunks.append(curr_chunk)\n",
    "            curr_chunk = ''\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "    \n",
    "def get_hsk_word_level(word):\n",
    "    if len(word) == 1:\n",
    "        return get_hanzi_level(word)\n",
    "    if word in hsk1_word_set:\n",
    "        return '1'\n",
    "    elif word in hsk2_word_set:\n",
    "        return '2'\n",
    "    elif word in hsk3_word_set:\n",
    "        return '3'\n",
    "    elif word in hsk4_word_set:\n",
    "        return '4'\n",
    "    elif word in hsk5_word_set:\n",
    "        return '5'\n",
    "    elif word in hsk6_word_set:\n",
    "        return '6'\n",
    "    elif word in hsk7_9_word_set:\n",
    "        return '7-9'\n",
    "    return 'Unknown'\n",
    "\n",
    "\n",
    "def word_in_hsk(word):\n",
    "    return word in hsk_word_set or word in hsk_hanzi_set\n",
    "\n",
    "'''\n",
    "# '法庭将于明年开庭审理他们涉嫌为中国从事间谍活动的案子'\n",
    "# sliding window parsing algorithm\n",
    "def tokenize_hanzi_string(hanzi_text):\n",
    "    tokens = []\n",
    "    text_length = len(hanzi_text)\n",
    "    curr_index = 1\n",
    "    \n",
    "    while hanzi_text != '':\n",
    "        curr_word = hanzi_text[:curr_index]\n",
    "        if not word_in_hsk(curr_word):\n",
    "            if curr_index > 1:\n",
    "                new_word = hanzi_text[:curr_index - 1]\n",
    "                hanzi_text = hanzi_text[curr_index - 1:]\n",
    "            else:\n",
    "                new_word = hanzi_text[:curr_index]\n",
    "                hanzi_text = hanzi_text[curr_index:]\n",
    "            new_word_level = get_hsk_word_level(new_word)\n",
    "            new_token = [new_word, new_word_level]\n",
    "            tokens.append(new_token)\n",
    "            curr_index = 1\n",
    "        else:\n",
    "            # if this is the last word to parse\n",
    "            if curr_word == hanzi_text:\n",
    "                new_word = hanzi_text[:curr_index]\n",
    "                hanzi_text = ''\n",
    "                new_word_level = get_hsk_word_level(new_word)\n",
    "                new_token = [new_word, new_word_level]\n",
    "                tokens.append(new_token)\n",
    "            curr_index += 1\n",
    "            \n",
    "    return tokens\n",
    "'''\n",
    "# for a string s, returns the length of the longest of s[:1], s[:2], s[:3], s[:4] that is an hsk word\n",
    "# if none of those prefix substrings are in the hsk word set, then it returns the shortest substring (s[:1])\n",
    "\n",
    "# in simpler terms, given a string, what is the prefix substring of the string that should be considered the next token\n",
    "def get_next_token_length(string):\n",
    "    next_possible_tokens = []\n",
    "    \n",
    "    for i in range(1, min([len(string) + 1, 5])):\n",
    "        next_possible_tokens.append(string[:i])\n",
    "        \n",
    "    while len(next_possible_tokens) > 0:\n",
    "        popped_token = next_possible_tokens.pop()\n",
    "        if get_hsk_word_level(popped_token) != 'Unknown':\n",
    "            return len(popped_token)\n",
    "        \n",
    "    return 1\n",
    "\n",
    "\n",
    "def tokenize_hanzi_string(string):\n",
    "    tokens = []\n",
    "    \n",
    "    while len(string) > 0:\n",
    "        next_index = get_next_token_length(string)\n",
    "        new_word = string[:next_index]\n",
    "        new_word_level = get_hsk_word_level(new_word)\n",
    "        new_word_pinyin = pinyin.get(new_word)\n",
    "        new_token = [new_word, new_word_level, new_word_pinyin]\n",
    "        tokens.append(new_token)\n",
    "        string = string[next_index:]\n",
    "        print(string)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    tokens = []\n",
    "    hanzi_chunks = split_into_hanzi_chunks(text)\n",
    "    non_hanzi_chunks = split_into_non_hanzi_chunks(text)\n",
    "    \n",
    "    if len(hanzi_chunks) == 0:\n",
    "        chunks = non_hanzi_chunks\n",
    "    elif len(non_hanzi_chunks) == 0:\n",
    "        chunks = hanzi_chunks\n",
    "    # interweave the two chunks depending on which type of chunk (hanzi or non-hanzi)\n",
    "    # appears in the text\n",
    "    elif is_hanzi(text[0]):\n",
    "        chunks = [chunk for chunk in itertools.chain(*itertools.zip_longest(hanzi_chunks, non_hanzi_chunks)) if chunk is not None]\n",
    "    else:\n",
    "        chunks = [chunk for chunk in itertools.chain(*itertools.zip_longest(non_hanzi_chunks, hanzi_chunks)) if chunk is not None]\n",
    "    \n",
    "    print(chunks)\n",
    "        \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # is this a hanzi chunk? If so, get tokens for it and append them to overall token list\n",
    "        if is_hanzi(chunk[0]):\n",
    "            chunk_tokens = tokenize_hanzi_string(chunk)\n",
    "            tokens.extend(chunk_tokens)\n",
    "        else:\n",
    "            non_hanzi_token = [chunk, 'Not Hanzi']\n",
    "            tokens.append(non_hanzi_token)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "080787a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_possible_tokens('为什么我的函数有这么大的问题')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "934c2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '华盛顿', ' — \\n', '中国在台湾总统赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测', '。', '美国多位军方官员此前对习近平攻台的时间表做出了种种预测', '。', '美国应该如何限制中国获得包括尖端芯片在内的关键技术', '，', '来阻遏北京武力攻台呢', '？', '前美国官员和技术专家认为', '，', '美国应该从现在就开始', '，', '采取一切必要行动确保中国对台湾的军事冒险不会得逞', '，', '包括推动多位盟友参与的复边出口控制体系', '，', '以及把现有的出口控制扩大到所有的芯片制造设备', '，', '使中国无法生产芯片']\n",
      "盛顿\n",
      "顿\n",
      "\n",
      "在台湾总统赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "台湾总统赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "湾总统赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "总统赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "引发台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "台海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "海局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "局势的紧张以及北京是否要武力夺取台湾的猜测\n",
      "的紧张以及北京是否要武力夺取台湾的猜测\n",
      "紧张以及北京是否要武力夺取台湾的猜测\n",
      "以及北京是否要武力夺取台湾的猜测\n",
      "北京是否要武力夺取台湾的猜测\n",
      "是否要武力夺取台湾的猜测\n",
      "要武力夺取台湾的猜测\n",
      "武力夺取台湾的猜测\n",
      "夺取台湾的猜测\n",
      "台湾的猜测\n",
      "湾的猜测\n",
      "的猜测\n",
      "猜测\n",
      "\n",
      "国多位军方官员此前对习近平攻台的时间表做出了种种预测\n",
      "多位军方官员此前对习近平攻台的时间表做出了种种预测\n",
      "位军方官员此前对习近平攻台的时间表做出了种种预测\n",
      "军方官员此前对习近平攻台的时间表做出了种种预测\n",
      "方官员此前对习近平攻台的时间表做出了种种预测\n",
      "官员此前对习近平攻台的时间表做出了种种预测\n",
      "此前对习近平攻台的时间表做出了种种预测\n",
      "对习近平攻台的时间表做出了种种预测\n",
      "习近平攻台的时间表做出了种种预测\n",
      "近平攻台的时间表做出了种种预测\n",
      "平攻台的时间表做出了种种预测\n",
      "攻台的时间表做出了种种预测\n",
      "台的时间表做出了种种预测\n",
      "的时间表做出了种种预测\n",
      "时间表做出了种种预测\n",
      "做出了种种预测\n",
      "出了种种预测\n",
      "了种种预测\n",
      "种种预测\n",
      "预测\n",
      "\n",
      "国应该如何限制中国获得包括尖端芯片在内的关键技术\n",
      "应该如何限制中国获得包括尖端芯片在内的关键技术\n",
      "如何限制中国获得包括尖端芯片在内的关键技术\n",
      "限制中国获得包括尖端芯片在内的关键技术\n",
      "中国获得包括尖端芯片在内的关键技术\n",
      "获得包括尖端芯片在内的关键技术\n",
      "包括尖端芯片在内的关键技术\n",
      "尖端芯片在内的关键技术\n",
      "芯片在内的关键技术\n",
      "在内的关键技术\n",
      "的关键技术\n",
      "关键技术\n",
      "技术\n",
      "\n",
      "阻遏北京武力攻台呢\n",
      "遏北京武力攻台呢\n",
      "北京武力攻台呢\n",
      "武力攻台呢\n",
      "攻台呢\n",
      "台呢\n",
      "呢\n",
      "\n",
      "美国官员和技术专家认为\n",
      "国官员和技术专家认为\n",
      "官员和技术专家认为\n",
      "和技术专家认为\n",
      "技术专家认为\n",
      "专家认为\n",
      "认为\n",
      "\n",
      "国应该从现在就开始\n",
      "应该从现在就开始\n",
      "从现在就开始\n",
      "现在就开始\n",
      "就开始\n",
      "开始\n",
      "\n",
      "一切必要行动确保中国对台湾的军事冒险不会得逞\n",
      "必要行动确保中国对台湾的军事冒险不会得逞\n",
      "行动确保中国对台湾的军事冒险不会得逞\n",
      "确保中国对台湾的军事冒险不会得逞\n",
      "中国对台湾的军事冒险不会得逞\n",
      "对台湾的军事冒险不会得逞\n",
      "台湾的军事冒险不会得逞\n",
      "湾的军事冒险不会得逞\n",
      "的军事冒险不会得逞\n",
      "军事冒险不会得逞\n",
      "冒险不会得逞\n",
      "不会得逞\n",
      "会得逞\n",
      "得逞\n",
      "逞\n",
      "\n",
      "推动多位盟友参与的复边出口控制体系\n",
      "多位盟友参与的复边出口控制体系\n",
      "位盟友参与的复边出口控制体系\n",
      "盟友参与的复边出口控制体系\n",
      "参与的复边出口控制体系\n",
      "的复边出口控制体系\n",
      "复边出口控制体系\n",
      "边出口控制体系\n",
      "出口控制体系\n",
      "控制体系\n",
      "体系\n",
      "\n",
      "把现有的出口控制扩大到所有的芯片制造设备\n",
      "现有的出口控制扩大到所有的芯片制造设备\n",
      "的出口控制扩大到所有的芯片制造设备\n",
      "出口控制扩大到所有的芯片制造设备\n",
      "控制扩大到所有的芯片制造设备\n",
      "扩大到所有的芯片制造设备\n",
      "到所有的芯片制造设备\n",
      "所有的芯片制造设备\n",
      "的芯片制造设备\n",
      "芯片制造设备\n",
      "制造设备\n",
      "设备\n",
      "\n",
      "中国无法生产芯片\n",
      "无法生产芯片\n",
      "生产芯片\n",
      "芯片\n",
      "\n",
      "0.0030007362365722656\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "parsed_text = parse_text('''\n",
    "华盛顿 — \n",
    "中国在台湾总统赖清德就任后举行环台军演的举动再次引发台海局势的紧张以及北京是否要武力夺取台湾的猜测。美国多位军方官员此前对习近平攻台的时间表做出了种种预测。美国应该如何限制中国获得包括尖端芯片在内的关键技术，来阻遏北京武力攻台呢？前美国官员和技术专家认为，美国应该从现在就开始，采取一切必要行动确保中国对台湾的军事冒险不会得逞，包括推动多位盟友参与的复边出口控制体系，以及把现有的出口控制扩大到所有的芯片制造设备，使中国无法生产芯片。\n",
    "''')\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "003ab99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\n', 'Not Hanzi'],\n",
       " ['华', '3', 'huá'],\n",
       " ['盛', '6', 'shèng'],\n",
       " ['顿', '3', 'dùn'],\n",
       " [' — \\n', 'Not Hanzi'],\n",
       " ['中国', '1', 'zhōngguó'],\n",
       " ['在', '1', 'zài'],\n",
       " ['台', '3', 'tái'],\n",
       " ['湾', '6', 'wān'],\n",
       " ['总统', '4', 'zǒngtǒng'],\n",
       " ['赖', '6', 'lài'],\n",
       " ['清', '2', 'qīng'],\n",
       " ['德', '5', 'dé'],\n",
       " ['就任', '7-9', 'jìurèn'],\n",
       " ['后', '1', 'hòu'],\n",
       " ['举行', '2', 'jǔxíng'],\n",
       " ['环', '3', 'huán'],\n",
       " ['台', '3', 'tái'],\n",
       " ['军', '5', 'jūn'],\n",
       " ['演', '3', 'yǎn'],\n",
       " ['的', '1', 'de'],\n",
       " ['举动', '5', 'jǔdòng'],\n",
       " ['再次', '5', 'zàicì'],\n",
       " ['引发', '7-9', 'yǐnfā'],\n",
       " ['台', '3', 'tái'],\n",
       " ['海', '2', 'hǎi'],\n",
       " ['局势', '7-9', 'júshì'],\n",
       " ['的', '1', 'de'],\n",
       " ['紧张', '3', 'jǐnzhāng'],\n",
       " ['以及', '4', 'yǐjí'],\n",
       " ['北京', '1', 'běijīng'],\n",
       " ['是否', '4', 'shìfǒu'],\n",
       " ['要', '1', 'yào'],\n",
       " ['武力', '7-9', 'wǔlì'],\n",
       " ['夺取', '6', 'duóqǔ'],\n",
       " ['台', '3', 'tái'],\n",
       " ['湾', '6', 'wān'],\n",
       " ['的', '1', 'de'],\n",
       " ['猜测', '5', 'cāicè'],\n",
       " ['。', 'Not Hanzi'],\n",
       " ['美', '3', 'měi'],\n",
       " ['国', '1', 'guó'],\n",
       " ['多', '1', 'duō'],\n",
       " ['位', '2', 'wèi'],\n",
       " ['军', '5', 'jūn'],\n",
       " ['方', '1', 'fāng'],\n",
       " ['官员', '7-9', 'guānyuán'],\n",
       " ['此前', '6', 'cǐqián'],\n",
       " ['对', '1', 'dùi'],\n",
       " ['习', '1', 'xí'],\n",
       " ['近', '2', 'jìn'],\n",
       " ['平', '2', 'píng'],\n",
       " ['攻', '6', 'gōng'],\n",
       " ['台', '3', 'tái'],\n",
       " ['的', '1', 'de'],\n",
       " ['时间表', '7-9', 'shíjiānbiǎo'],\n",
       " ['做', '1', 'zuò'],\n",
       " ['出', '1', 'chū'],\n",
       " ['了', '1', 'le'],\n",
       " ['种种', '6', 'zhǒngzhǒng'],\n",
       " ['预测', '4', 'yùcè'],\n",
       " ['。', 'Not Hanzi'],\n",
       " ['美', '3', 'měi'],\n",
       " ['国', '1', 'guó'],\n",
       " ['应该', '2', 'yìnggāi'],\n",
       " ['如何', '3', 'rúhé'],\n",
       " ['限制', '4', 'xiànzhì'],\n",
       " ['中国', '1', 'zhōngguó'],\n",
       " ['获得', '4', 'huòdé'],\n",
       " ['包括', '4', 'bāokuò'],\n",
       " ['尖端', '7-9', 'jiānduān'],\n",
       " ['芯片', '7-9', 'xīnpiàn'],\n",
       " ['在内', '5', 'zàinèi'],\n",
       " ['的', '1', 'de'],\n",
       " ['关键', '5', 'guānjiàn'],\n",
       " ['技术', '3', 'jìzhú'],\n",
       " ['，', 'Not Hanzi'],\n",
       " ['来', '1', 'lái'],\n",
       " ['阻', '4', 'zǔ'],\n",
       " ['遏', '7-9', 'è'],\n",
       " ['北京', '1', 'běijīng'],\n",
       " ['武力', '7-9', 'wǔlì'],\n",
       " ['攻', '6', 'gōng'],\n",
       " ['台', '3', 'tái'],\n",
       " ['呢', '1', 'ní'],\n",
       " ['？', 'Not Hanzi'],\n",
       " ['前', '1', 'qián'],\n",
       " ['美', '3', 'měi'],\n",
       " ['国', '1', 'guó'],\n",
       " ['官员', '7-9', 'guānyuán'],\n",
       " ['和', '1', 'hé'],\n",
       " ['技术', '3', 'jìzhú'],\n",
       " ['专家', '3', 'zhuānjiā'],\n",
       " ['认为', '2', 'rènwèi'],\n",
       " ['，', 'Not Hanzi'],\n",
       " ['美', '3', 'měi'],\n",
       " ['国', '1', 'guó'],\n",
       " ['应该', '2', 'yìnggāi'],\n",
       " ['从', '1', 'cóng'],\n",
       " ['现在', '1', 'xiànzài'],\n",
       " ['就', '1', 'jìu'],\n",
       " ['开始', '3', 'kāishǐ'],\n",
       " ['，', 'Not Hanzi'],\n",
       " ['采取', '3', 'cǎiqǔ'],\n",
       " ['一切', '3', 'yīqiē'],\n",
       " ['必要', '3', 'bìyào'],\n",
       " ['行动', '2', 'xíngdòng'],\n",
       " ['确保', '3', 'quèbǎo'],\n",
       " ['中国', '1', 'zhōngguó'],\n",
       " ['对', '1', 'dùi'],\n",
       " ['台', '3', 'tái'],\n",
       " ['湾', '6', 'wān'],\n",
       " ['的', '1', 'de'],\n",
       " ['军事', '6', 'jūnshì'],\n",
       " ['冒险', '7-9', 'màoxiǎn'],\n",
       " ['不', '1', 'bù'],\n",
       " ['会', '1', 'hùi'],\n",
       " ['得', '1', 'dé'],\n",
       " ['逞', '7-9', 'chěng'],\n",
       " ['，', 'Not Hanzi'],\n",
       " ['包括', '4', 'bāokuò'],\n",
       " ['推动', '3', 'tūidòng'],\n",
       " ['多', '1', 'duō'],\n",
       " ['位', '2', 'wèi'],\n",
       " ['盟友', '7-9', 'méngyǒu'],\n",
       " ['参与', '4', 'cānyǔ'],\n",
       " ['的', '1', 'de'],\n",
       " ['复', '2', 'fù'],\n",
       " ['边', '1', 'biān'],\n",
       " ['出口', '2', 'chūkǒu'],\n",
       " ['控制', '5', 'kòngzhì'],\n",
       " ['体系', '7-9', 'tǐxì'],\n",
       " ['，', 'Not Hanzi'],\n",
       " ['以及', '4', 'yǐjí'],\n",
       " ['把', '3', 'bǎ'],\n",
       " ['现有', '5', 'xiànyǒu'],\n",
       " ['的', '1', 'de'],\n",
       " ['出口', '2', 'chūkǒu'],\n",
       " ['控制', '5', 'kòngzhì'],\n",
       " ['扩大', '4', 'kuòdà'],\n",
       " ['到', '1', 'dào'],\n",
       " ['所有', '2', 'suǒyǒu'],\n",
       " ['的', '1', 'de'],\n",
       " ['芯片', '7-9', 'xīnpiàn'],\n",
       " ['制造', '3', 'zhìzào'],\n",
       " ['设备', '3', 'shèbèi'],\n",
       " ['，', 'Not Hanzi'],\n",
       " ['使', '2', 'shǐ'],\n",
       " ['中国', '1', 'zhōngguó'],\n",
       " ['无法', '4', 'wúfǎ'],\n",
       " ['生产', '3', 'shēngchǎn'],\n",
       " ['芯片', '7-9', 'xīnpiàn']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aacd4825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_token_length('芯片')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b92d8cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'迫不得已' in hsk_word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dfdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
